# Internship-avito-backend

## Основной стек
- Golang 1.23.6
- net/http
- go-sqlmock
- squirrel
- chi
- jwt/v4
- prometheus
- testify
- protobuf
- slog
- testify/mock

## Конфигурация
Проект использует следующие переменные окружения (`.env`):
- `POSTGRES_USER` - пользователь БД
- `POSTGRES_PASSWORD` - пароль БД
- `POSTGRES_DB` - имя базы данных
- `SERVER_PORT` - порт HTTP сервера
- `METRICS_PORT` - порт для метрик Prometheus
- `JWT_SIGNING_KEY` - ключ для подписи JWT токенов
- `JWT_TOKEN_DURATION` - время жизни токена
- `POSTGRES_TEST_HOST` - хост для тестовой базы


Перед запуском проекта следует создать `.env` на основе `.env.example`. Пример уже предзаполнен тестовыми данными для быстрого запуска докера, поэтому впринципе можно его просто переименовать, убрав .example

## Запуск

Запуск контейнера

1) Через Makefile(не в daemon режиме, для просмотра логов сервера)
```bash
make up
```
2) Через Makefile(в daemon режиме, для прогона тестов)
```bash
make up-daemon
```
3) Напрямую
```bash
docker-compose up --build
```
```bash
docker compose up --build -d
```

В результате запуска мы получаем  
- Основной сервер на :8080  
- Сервер метрик на :9000  
- gRPC сервер на :3000  
- Основную бд (с автоматическим применением миграций для большего удобства)
- Тестовую бд (для прогона интеграционных тестов)

## Роутинг
Роуты совпадают с описанием [Swagger](swagger.yaml)  
Протофайлы для теста gRPC находятся в директории [grpc](src/internal/delivery/grpc/pvz.proto)  
`src/internal/delivery/grpc/pvz.proto`

### Эндпоинты для аутентификации

POST http://localhost:8080/register - Регистрация нового пользователя.  
POST http://localhost:8080/login - Вход пользователя в систему.  
POST http://localhost:8080/dummyLogin - Псевдологин для тестирования.  
### Эндпоинты для работы с PVZ  

#### Роли: ModeratorRole   
POST http://localhost:8080/pvz - Создание нового PVZ.  

#### Роли: EmployeeRole  

POST http://localhost:8080/receptions - Создание новой приемки.  
POST http://localhost:8080/products - Создание нового продукта.  
POST http://localhost:8080/pvz/{pvzId}/delete_last_product - Удаление последнего продукта из PVZ.  
POST http://localhost:8080/pvz/{pvzId}/close_last_reception - Закрытие последней приемки в PVZ.  

#### Роли: EmployeeRole и ModeratorRole  

GET http://localhost:8080/pvz - Получение списка PVZ.

### Эндпоинт метрик
GET http://localhost:9000/metrics

### gRPC Эндпоинт
localhost:3000 - Метод GetPVZList


## Тестирование 
Unit тесты лежат рядом с тестируемым функционалом, если их несколько, то они упаковы в дерикторию tests. Например [HandlersUnitTests](src/internal/delivery/http/handlers/tests/)  
Для интеграционных тестов создана отдельная папка [integration](src/tests/integration/)

<b>Запуск тестов</b>  
Запуск тестов реализован внутри докера, чтобы не тащить зависимости на пк

Через Makefile (Требует сначала поднять docker-compose)

```bash
make test-unit
```

```bash
make test-integration
```

Вручную (юнит и интеграционные соответсвенно)
```bash
docker-compose exec -T app go test ./...
```


```bash
docker-compose exec -T app go test -tags=integration ./src/tests/integration/...
```

Юнит тесты можно запустить вне контейнера, тогда они просто установят зависимости в локальную среду
```bash
go test ./...
```

Интеграционные тесты требуют развернутую бд и приложение(оно тянет .env), поэтому только через докер  

## Покрытие тестами
```bash
go test -coverprofile=coverage.out ./...
```
```bash
go tool cover -func=coverage.out
```

На данный момент покрытие с учетом сгенерированных protobuf файлов составляет 75.8%
```
total:	(statements)						75.8%
```

Я не знаю, должны ли они идти в общий зачет по покрытию, ведь напрямую я не могу их тестировать, только связанные сервисы, поэтому вот покрытие без учета сгенерованных файлов - 83.1%
```bash
go list ./... | grep -v 'pb$' | xargs go test -coverprofile=coverage.out
```
```bash
go tool cover -func=coverage.out
```

```
total:	(statements)			83.1%
```

## Логирование
- Все логи пишутся в структурированном формате с использованием `slog`
- Логи сохраняются в файл `logs/app.log` и дублируются в stdout
- В логах отслеживается `request_id` для каждого запроса
- Бизнес-события логируются на русском языке
- Конфиденциальные данные (пароли) не попадают в логи



## Чеклист
- [x] Авторизация пользователей
- [x] Регистрация и авторизация пользователей по почте и паролю
- [x] Заведение ПВЗ
- [x] Добавление информации о приёмке товаров
- [x] Добавление товаров в рамках одной приёмки
- [x] Удаление товаров в рамках не закрытой приёмки
- [x] Закрытие приёмки
- [x] Получение данных о всех ПВЗ
- [x] Реализовать gRPC-метод
- [x] Добавить в проект prometheus
- [x] Настроить логирование в проекте

## Уточнения/Вопросы по проекту
1) В Swagger описании ручки /login нет указаний о статус коде ответа при неверном(невалидном) JSON, поэтому был выбран логичный вариант - 400
2) В Swagger описании всех авторизованных ручек нет указаний о статус коде отсутсвия токена при запросе, есть только запрет доступа(403), поэтому был выбран статус код 401, ведь он точнее отображает ситуацию. Этот статус-код указывает, что запрос требует аутентификации пользователя. Если пользователь не предоставил действительные учетные данные (например, имя пользователя и пароль или токен).
3) Не совсем понял для чего в ручке создания ПВЗ в example value присутсвуют id и registrationDate, ведь это автогенерируемые поля(хоть и указано required: [city])  
Поэтому в моей реализации из запроса берется только город, всё остальное сервер создает сам(Если из под свагера кинуть запрос с этими полями, он все равно сработает, просто не учтет их).  
<b>Если я неправильно понял, попрошу сильно не бить :)</b>
4) В Swagger описании ручки закрытия приемки в example value статус у приемки "in_progress", хотя если ответ пришел, то логично, что приемка уже закрыта  
Поэтому в моей реализации статус у ответа "close"



